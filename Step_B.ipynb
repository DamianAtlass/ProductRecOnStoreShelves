{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step B - Multiple Instance Detection:\n",
    "In addition to what achieved at step A, the system should now be able to detect multiple instance of the\n",
    "same product. Purposely, students may deploy local invariant feature together with the GHT (Generalized\n",
    "Hough Transform). More precisely, rather than relying on the usual R-Table, the object model acquired at\n",
    "training time should now consist in vectors joining all the features extracted in the model image to their\n",
    "barycenter; then, at run time all the image features matched with respect to the model would cast votes\n",
    "for the position of the barycenter by scaling appropriately the associated joining vectors (i.e. by the ratio of\n",
    "sizes between the matching features)."
   ],
   "id": "59df145625f1eeac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:50:26.237125Z",
     "start_time": "2024-12-13T13:50:25.984200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt"
   ],
   "id": "3748176478e041d5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-12-13T13:48:36.968678Z",
     "start_time": "2024-12-13T13:04:07.699757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img = cv.imread('product-recognition-on-store-shelves-images/object_detection_project/models/0.jpg',cv.IMREAD_GRAYSCALE)\n",
    "scene = cv.imread('product-recognition-on-store-shelves-images/object_detection_project/scenes/m1.png',cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "height, width = img.shape[:2]\n",
    "\n",
    "edges = cv.Canny(img, 200, 250)\n",
    "ght = cv.createGeneralizedHoughGuil()\n",
    "ght.setTemplate(edges)\n",
    "\n",
    "ght.setMinDist(100)\n",
    "ght.setMinAngle(0)\n",
    "ght.setMaxAngle(360)\n",
    "ght.setAngleStep(1)\n",
    "ght.setLevels(360)\n",
    "ght.setMinScale(1)\n",
    "ght.setMaxScale(1.3)\n",
    "ght.setScaleStep(0.05)\n",
    "ght.setAngleThresh(100)\n",
    "ght.setScaleThresh(100)\n",
    "ght.setPosThresh(100)\n",
    "ght.setAngleEpsilon(1)\n",
    "ght.setLevels(360)\n",
    "ght.setXi(90)\n",
    "\n",
    "positions = ght.detect(scene)[0][0]"
   ],
   "id": "b0f87a793bf0a509",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for position in positions:\n",
    "    center_col = int(position[0])\n",
    "    center_row = int(position[1])\n",
    "    scale = position[2]\n",
    "    angle = int(position[3])\n",
    "\n",
    "    found_height = int(height * scale)\n",
    "    found_width = int(width * scale)\n",
    "\n",
    "    rectangle = ((center_col, center_row),\n",
    "                 (found_width, found_height),\n",
    "                 angle)\n",
    "\n",
    "    box = cv.boxPoints(rectangle)\n",
    "    box = np.int0(box)\n",
    "    cv.drawContours(img, [box], 0, (0, 0, 255), 2)\n",
    "\n",
    "    for i in range(-2, 3):\n",
    "        for j in range(-2, 3):\n",
    "            img[center_row + i, center_col + j] = 0, 0, 255\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ],
   "id": "700749c98953dc9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:53:15.085405Z",
     "start_time": "2024-12-13T13:52:19.525207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the images\n",
    "#image = cv2.imread('shelf.jpg')  # Input shelf image\n",
    "#template = cv2.imread('cereal_box.jpg')  # Template of cereal box\n",
    "image_gray = cv.imread('product-recognition-on-store-shelves-images/object_detection_project/models/0.jpg',cv.IMREAD_GRAYSCALE)\n",
    "template_gray = cv.imread('product-recognition-on-store-shelves-images/object_detection_project/scenes/m1.png',cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Convert to grayscale\n",
    "#image_gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "#template_gray = cv.cvtColor(template, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Edge detection\n",
    "image_edges = cv.Canny(image_gray, 50, 150)\n",
    "template_edges = cv.Canny(template_gray, 50, 150)\n",
    "\n",
    "# Create a Generalized Hough Transform object\n",
    "ght = cv.createGeneralizedHoughBallard()\n",
    "\n",
    "# Set the template\n",
    "ght.setTemplate(template_edges)\n",
    "\n",
    "# Detect the object\n",
    "positions, _ = ght.detect(image_edges)\n",
    "\n",
    "# Draw detections\n",
    "for position in positions:\n",
    "    x, y, scale, rotation = position\n",
    "    cv.circle(image_gray, (int(x), int(y)), 10, (0, 255, 0), 3)\n",
    "\n",
    "# Show the result\n",
    "cv.imshow('Detected Cereal Boxes', image_gray)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ],
   "id": "4f6fc4776ff7820d",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 29\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Draw detections\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m position \u001B[38;5;129;01min\u001B[39;00m positions:\n\u001B[0;32m---> 29\u001B[0m     x, y, scale, rotation \u001B[38;5;241m=\u001B[39m position\n\u001B[1;32m     30\u001B[0m     cv\u001B[38;5;241m.\u001B[39mcircle(image_gray, (\u001B[38;5;28mint\u001B[39m(x), \u001B[38;5;28mint\u001B[39m(y)), \u001B[38;5;241m10\u001B[39m, (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Show the result\u001B[39;00m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_312_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_312_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_312_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1220\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1217\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1219\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1220\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1235\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1232\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1234\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1235\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[1;32m   1237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1239\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
