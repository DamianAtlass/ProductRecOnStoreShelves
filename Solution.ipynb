{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Universal",
   "id": "ecb51f3d743470bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import os"
   ],
   "id": "f7d8e4ca07fad1af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These are some convenience function for plotting and loading data.",
   "id": "20a95f800e4333c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#convenience function\n",
    "path_models = os.path.join(\"product-recognition-on-store-shelves-images\",\"object_detection_project\",\"models\")\n",
    "path_scenes = os.path.join(\"product-recognition-on-store-shelves-images\",\"object_detection_project\",\"scenes\")\n",
    "\n",
    "def get_path_model(index):\n",
    "    return os.path.join(path_models, f\"{index}.jpg\")\n",
    "\n",
    "def get_path_scene(index):\n",
    "    return os.path.join(path_scenes, f\"{index}\")\n",
    "\n",
    "def rotate_tuple(point, angle_degrees):\n",
    "    # Convert angle to radians\n",
    "    angle_radians = math.radians(angle_degrees)\n",
    "\n",
    "    # Extract the x and y coordinates\n",
    "    x, y = point\n",
    "\n",
    "    # Apply the rotation matrix\n",
    "    x_rotated = x * math.cos(angle_radians) - y * math.sin(angle_radians)\n",
    "    y_rotated = x * math.sin(angle_radians) + y * math.cos(angle_radians)\n",
    "\n",
    "    return (x_rotated, y_rotated)\n",
    "\n",
    "def is_point_in_rectangle(p, rect):\n",
    "\n",
    "    def vector(a, b):\n",
    "        return (b[0] - a[0], b[1] - a[1])\n",
    "\n",
    "    def dot_product(v1, v2):\n",
    "        return v1[0] * v2[0] + v1[1] * v2[1]\n",
    "\n",
    "    def magnitude_squared(v):\n",
    "        return v[0]**2 + v[1]**2\n",
    "\n",
    "    # Unpack rectangle vertices\n",
    "    A, B, C, D = rect\n",
    "\n",
    "    # Compute vectors\n",
    "    AB = vector(A, B)\n",
    "    AD = vector(A, D)\n",
    "    AP = vector(A, p)\n",
    "\n",
    "    # Check if the point projects within both sides of the rectangle\n",
    "    within_AB = 0 <= dot_product(AP, AB) <= magnitude_squared(AB)\n",
    "    within_AD = 0 <= dot_product(AP, AD) <= magnitude_squared(AD)\n",
    "\n",
    "    return within_AB and within_AD\n",
    "\n",
    "#drawing functions\n",
    "def plot_images(image_paths):\n",
    "\n",
    "    images = [cv.imread(p) for p in image_paths]\n",
    "\n",
    "    # Check if the list is empty\n",
    "    if not images:\n",
    "        raise ValueError(\"The list of images is empty.\")\n",
    "\n",
    "    # Find the maximum height among all images\n",
    "    max_height = max(img.shape[0] for img in images)\n",
    "\n",
    "    # Resize all images to have the same height\n",
    "    resized_images = [\n",
    "        cv.resize(img, (int(img.shape[1] * max_height / img.shape[0]), max_height))\n",
    "        for img in images\n",
    "    ]\n",
    "\n",
    "    # Horizontally concatenate images\n",
    "    concatenated_image = np.hstack(resized_images)\n",
    "    print([os.path.split(p)[-1] for p in image_paths])\n",
    "    plt.imshow(cv.cvtColor(concatenated_image, cv.COLOR_BGR2RGB))\n",
    "    plt.title(\"models\")\n",
    "    plt.show()\n",
    "\n",
    "def draw_good_and_bad_matches(scene, good, bad, kp_scene):\n",
    "#draw excluded and included matches\n",
    "        for o in bad:\n",
    "            x,y = kp_scene[o.trainIdx].pt\n",
    "            coord = [int(x), int(y)]\n",
    "            cv.circle(scene, coord, 3, (0,0,255), 2)\n",
    "\n",
    "        for o in good:\n",
    "            x,y = kp_scene[o.trainIdx].pt  # Get match coordinates\n",
    "            coord = [int(x), int(y)]\n",
    "            cv.circle(scene, coord, 3, (0,255,0), 2)\n",
    "\n",
    "        plt.imshow(cv.cvtColor(scene, cv.COLOR_BGR2RGB))\n",
    "        plt.title(\"Good and Bad (rejected) matches\")\n",
    "        plt.show()\n",
    "\n",
    "def draw_matches(model, kp_model, scene, kp_scene, good):\n",
    "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                        singlePointColor = None, # not draw keypoints only matching lines\n",
    "                        flags = 2) # not draw keypoints only lines\n",
    "    image = cv.drawMatches(model, kp_model, scene, kp_scene, good, None,**draw_params)\n",
    "    plt.title(\"Matched keypoints\")\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "def draw_barycenter_predictions(scene, barygrid_cell_width, barygrid_cell_height, barycenter_votes, dst_kp):\n",
    "\n",
    "    for b, d in zip(barycenter_votes, dst_kp):\n",
    "        d = [int(d.pt[0]), int(d.pt[1])]\n",
    "        cv.circle(scene, d, 3, (0,255,0), 4)\n",
    "        cv.circle(scene, b, 3, (255,0,0), 4)\n",
    "        cv.line(scene, d, b, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    height, width, _ = scene.shape\n",
    "    num_vertical_lines = width // barygrid_cell_width\n",
    "    num_horizontal_lines = height // barygrid_cell_height\n",
    "\n",
    "    for i in range(1, num_vertical_lines):\n",
    "        x = i * barygrid_cell_width\n",
    "        cv.line(scene, (x, 0), (x, height), color=(0, 0, 255), thickness=2)\n",
    "\n",
    "    for i in range(1, num_horizontal_lines+1):\n",
    "        y = i * barygrid_cell_height\n",
    "        cv.line(scene, (0, y), (width, y), color=(0, 0, 255), thickness=2)\n",
    "\n",
    "    plt.imshow(cv.cvtColor(scene, cv.COLOR_BGR2RGB))\n",
    "    plt.title(\"Features with their respective joining vectors (green), bins and votes (blue)\")\n",
    "    plt.show()"
   ],
   "id": "869fbe498a791536",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To simplify handling the detected products, we decidde to use define a class to wrapp them into a single data structure.",
   "id": "48f2deb0e43c9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Product:\n",
    "  def __init__(self, position, width, height, cornerpoints):\n",
    "    self.position = position\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.cornerpoints = cornerpoints\n",
    "\n",
    "\"\"\"def product_recognition(scene, models, result):\n",
    "    scene_img = cv.imread(get_path_scene(scene))\n",
    "    for i in range(len(models)):\n",
    "        if result[i] is not None:\n",
    "            print(f\"Product {models[i]} instance found:\")\n",
    "            print(\"\\tInstance {}: position: {}, width: {}px, height: {}px\".format(i,\n",
    "                                                                                  result[i].position,\n",
    "                                                                                  result[i].width,\n",
    "                                                                                  result[i].height) )\n",
    "            cv.rectangle(scene_img,\n",
    "                         (result[i].position[0] - int(result[i].width/2),\n",
    "                          result[i].position[1] - int(result[i].height/2)),\n",
    "                         (result[i].position[0] + int(result[i].width/2),\n",
    "                          result[i].position[1] + int(result[i].height/2)),\n",
    "                         (0,255,0),3)\n",
    "\n",
    "            #add square and add text\n",
    "            means = np.mean(result[i].cornerpoints, axis=0)\n",
    "            x, y = [a for a in means[0]]\n",
    "            x = x * 0.9 #adjust for text\n",
    "            boldness = 3\n",
    "            size = 3\n",
    "\n",
    "            cv.putText(scene_img, str(models[i]), (int(x),int(y)), cv.FONT_HERSHEY_COMPLEX_SMALL, size, (0, 0, 255, 255), boldness, cv.LINE_AA)\n",
    "\n",
    "    plt.imshow(cv.cvtColor(scene_img, cv.COLOR_BGR2RGB))\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "def product_recognition(scene, models, result):\n",
    "    scene_gray = cv.imread(get_path_scene(scene))\n",
    "    for i in range(len(result)):\n",
    "        product_results = result[i]\n",
    "        print(f\"Product {models[i]} - {len(product_results)} instance(s) found:\")\n",
    "        for j in range(len(product_results)):\n",
    "            if product_results[j] is not None:\n",
    "                print(\"\\tInstance {}: position: {}, width: {}px, height: {}px\".format(j,\n",
    "                                                                                      product_results[j].position,\n",
    "                                                                                      product_results[j].width,\n",
    "                                                                                      product_results[j].height) )\n",
    "                cv.rectangle(scene_gray,\n",
    "                             (product_results[j].position[0] - int(product_results[j].width/2),\n",
    "                              product_results[j].position[1] - int(product_results[j].height/2)),\n",
    "                             (product_results[j].position[0] + int(product_results[j].width/2),\n",
    "                              product_results[j].position[1] + int(product_results[j].height/2)),\n",
    "                             (0,255,0),3)\n",
    "\n",
    "                #add square and add text\n",
    "                means = np.mean(product_results[j].cornerpoints, axis=0)\n",
    "                x, y = [a for a in means[0]]\n",
    "                x = x * 0.9 #adjust for text\n",
    "                boldness = 3\n",
    "                size = 3\n",
    "\n",
    "                cv.putText(scene_gray, str(models[j]), (int(x),int(y)), cv.FONT_HERSHEY_COMPLEX_SMALL, size, (0, 0, 255, 255), boldness, cv.LINE_AA)\n",
    "\n",
    "    plt.imshow(cv.cvtColor(scene_gray, cv.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ],
   "id": "e82f8b7fea6a39da",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f4c21b5fca783a2e",
   "metadata": {},
   "source": "# Step A - Multiple Product Detection:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "The Task of Step A:\n",
    "\n",
    "\"Develop an object detection system to identify single instance of products given: one reference image for\n",
    "each item and a scene image. The system should be able to correctly identify all the product in the shelves\n",
    "image. One way to solve this task could be the use of local invariant feature as explained in lab session 5.\"\n",
    "\n",
    "As proposed in the task description, we will utilize local invariant features to match a reference model with a scene image. This approach involves detecting keypoints and descriptors using SIFT, then matching them using the k-nearest neighbors algorithm. To filter out incorrect matches, we apply Lowe’s ratio test, retaining only the best matches. Finally, the remaining matches are used to determine the location of the model in the scene."
   ],
   "id": "9e5265599e677466"
  },
  {
   "cell_type": "markdown",
   "id": "e32ac0a06a535549",
   "metadata": {},
   "source": [
    "### Source\n",
    "https://docs.opencv.org/3.4/d1/de0/tutorial_py_feature_homography.html\n",
    "\n",
    "https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16258dd95f006d47",
   "metadata": {},
   "source": [
    "## Example Execution\n",
    "In this first example, we will only try to locate cereal box 0 in scene e1 for simplicity’s sake."
   ]
  },
  {
   "cell_type": "code",
   "id": "79ee8a2b7cfaa86b",
   "metadata": {},
   "source": [
    "scene_path = get_path_scene(\"e1.png\")\n",
    "model_path = get_path_model(0)\n",
    "plot_diagrams=False\n",
    "\n",
    "if plot_diagrams:\n",
    "    plot_images(model_path)\n",
    "\n",
    "scene_gray = cv.imread(scene_path, cv.IMREAD_GRAYSCALE)\n",
    "scene_to_draw_on = cv.imread(scene_path)\n",
    "model_gray = cv.imread(model_path, cv.IMREAD_GRAYSCALE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5d447775aac0dec",
   "metadata": {},
   "source": [
    "## SIFT (Scale Invariant Feature Transform)\n",
    "In this step we use SIFT to determine both the Keypoint's and descriptors of both the model and the scene. Sift works by following a few steps;\n",
    "1. Keypoint Detection: SIFT determines keypoint locations using the Difference of Gaussians (DoG) function. A potential keypoint is identified as a local extremum in both scale and space within the DoG representation.\n",
    "2. Keypoint Refinement: The initial set of keypoints includes many unstable points, which need to be refined. To improve accuracy, Taylor series expansion is used to localize keypoints more precisely. Keypoints found along edges are often unstable, so another pruning step based on the eigenvalues of the Hessian is necessary. If the curvature ratio exceeds a certain threshold, the keypoint is discarded.\n",
    "3. Orientation Assignment: To ensure invariance to image rotation, each keypoint is assigned a canonical orientation. The keypoint’s neighborhood is divided into an orientation histogram with 36 bins, each covering 10 degrees. The dominant gradient direction is selected as the keypoint’s orientation, and any additional peaks above 80% of the maximum are also considered.\n",
    "4. Descriptor Computation: Once keypoints are detected, descriptors are computed based on the surrounding area. The local region around each keypoint is divided into a grid, typically forming a 4×4 array of orientation histograms, with each histogram having 8 orientation bins. This results in a feature vector of 128 dimensions, which represents the keypoint descriptor.\n",
    "\n",
    "Source: https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html"
   ]
  },
  {
   "cell_type": "code",
   "id": "73833de56c551304",
   "metadata": {},
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp_model, des_model = sift.detectAndCompute(model_gray,None)\n",
    "kp_scene, des_scene = sift.detectAndCompute(scene_gray,None)\n",
    "\n",
    "img_visualization = cv.drawKeypoints(model_gray,kp_model,None,flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "print(\"Visualization of keypoints in model image:\")\n",
    "plt.imshow(img_visualization)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "561850ae323bf63d",
   "metadata": {},
   "source": [
    "## Keypoint Matching\n",
    "To match keypoints between the model and the scene, a nearest-neighbor algorithm is applied to the descriptors. However, a challenge arises when the difference between the best match and the second-best match is small, because this implies a low accuracy of the first match.\n",
    "\n",
    "To reduce false matches, Lowe’s ratio test is used. This test computes the ratio between the best match and the second-best match. If the ratio is above a certain threshold, the match is rejected, as it suggests ambiguity. Lowe originally proposed a threshold of 0.8, but in our case, we use 0.7, as it was used in Lab Session 5.\n",
    "\n",
    "Source: https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html"
   ]
  },
  {
   "cell_type": "code",
   "id": "5881a6d80a8f1286",
   "metadata": {},
   "source": [
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "flann = cv.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des_model,des_scene,k=2)\n",
    "\n",
    "# store all the good matches as per Lowe's ratio test.\n",
    "good = []\n",
    "bad = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append(m)\n",
    "    else:\n",
    "        bad.append(m)\n",
    "\n",
    "draw_good_and_bad_matches(scene_to_draw_on, good, bad, kp_scene)\n",
    "draw_matches(model_gray, kp_model, scene_gray, kp_scene, good)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f257893e22be84d2",
   "metadata": {},
   "source": [
    "## Homography and Perspective Transform\n",
    "Once the model has been matched with the scene, the next step is to compute the homography, which describes how the model image has been scaled, rotated, and transformed in the scene.\n",
    "\n",
    "The homography matrix can be derived from the matched keypoints. Using this transformation, OpenCV’s cv2.perspectiveTransform() function can be applied to find the coordinates of the four corner points of the model (e.g., a cereal box) in the scene.\n",
    "\n",
    "During this process, it is important to note that just because a model produces a few matches, it does not necessarily mean it is present in the scene. To address this issue, a simple threshold on the minimum number of matches can be applied.\n",
    "\n",
    "Setting an appropriate threshold can be challenging for large datasets, but in this case, where only a small number of models and scenes are considered, manually choosing a threshold is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "id": "362444553231888",
   "metadata": {},
   "source": [
    "MIN_MATCH_COUNT = 200\n",
    "scene_to_draw_on = cv.imread(scene_path)\n",
    "\n",
    "if len(good)>MIN_MATCH_COUNT:\n",
    "    print( \"Enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT) )\n",
    "    src_pts = np.float32([ kp_model[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp_scene[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC,5.0)\n",
    "    h,w = model_gray.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "\n",
    "    dst = cv.perspectiveTransform(pts,M)\n",
    "\n",
    "    #add square and add text\n",
    "    means = np.mean(dst, axis=0)\n",
    "    x, y = [a for a in means[0]]\n",
    "    x = x * 0.8 #adjust for text\n",
    "\n",
    "    result = cv.polylines(scene_to_draw_on,[np.int32(dst)],True,255,3, cv.LINE_AA)\n",
    "\n",
    "    boldness = 3\n",
    "    size = 3\n",
    "    cv.putText(scene_to_draw_on, \"0.jpg\", (int(x),int(y)), cv.FONT_HERSHEY_COMPLEX_SMALL, size, (0, 0, 255, 255), boldness, cv.LINE_AA)\n",
    "\n",
    "else:\n",
    "    print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT) )\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Result:\")\n",
    "plt.imshow(cv.cvtColor(result, cv.COLOR_BGR2RGB))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5f16352a806577a8",
   "metadata": {},
   "source": [
    "## Solution\n",
    "In the following section, all proposed steps will be bundled into a single function, which takes multiple models and a scene as input and returns the locations of the models in the scene. Additionally, all models are scaled to the same size, matching the smallest model, since we found that an  incorrect model with a high resolution could sometimes produce more matches than the correct model with a lower resolution.\n",
    "\n",
    "The following code demonstrates that this approach successfully locates the model in simple scenes. However, as scene complexity increases (e.g., when two identical cereal boxes are present), the method begins to struggle. That’s why, in Step B, we will improve this approach by utilizing the Generalized Hough Transform."
   ]
  },
  {
   "cell_type": "code",
   "id": "966553f8f41a5d7f",
   "metadata": {},
   "source": [
    "def find_image_A(scene_path, model_paths):\n",
    "    scene_gray = cv.imread(scene_path,cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "    #create array with all models\n",
    "    models = []\n",
    "    for model_gray_path in model_paths:\n",
    "        name = os.path.split(model_gray_path)[-1]\n",
    "        models.append({\n",
    "            \"name\": name,\n",
    "            \"img\": cv.imread(model_gray_path, cv.IMREAD_GRAYSCALE)\n",
    "            })\n",
    "\n",
    "    common_height = np.min([o[\"img\"].shape[0] for o in models])\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in models:\n",
    "        model_gray = model[\"img\"]\n",
    "        ratio = common_height/model_gray.shape[0]\n",
    "        new_w = model_gray.shape[1]*ratio\n",
    "        model_gray = cv.resize(model_gray, (int(new_w), common_height), interpolation=cv.INTER_AREA)\n",
    "        MIN_MATCH_COUNT = 200\n",
    "\n",
    "        # Initiate SIFT detector\n",
    "        sift = cv.SIFT_create()\n",
    "\n",
    "        # find the keypoints and descriptors with SIFT\n",
    "        kp_model, des_model = sift.detectAndCompute(model_gray,None)\n",
    "        kp_scene, des_scene = sift.detectAndCompute(scene_gray,None)\n",
    "\n",
    "        #MIN_MATCH_PERCENTAGE_THRESHOLD = int(len(kp_model) * MIN_MATCH_PERCENTAGE/100)\n",
    "\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks = 50)\n",
    "        flann = cv.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = flann.knnMatch(des_model,des_scene,k=2)\n",
    "\n",
    "        # store all the good matches as per Lowe's ratio test.\n",
    "        good = []\n",
    "        bad = []\n",
    "        for m,n in matches:\n",
    "            if m.distance < 0.7 * n.distance:\n",
    "                good.append(m)\n",
    "            else:\n",
    "                bad.append(m)\n",
    "\n",
    "        if len(good)>MIN_MATCH_COUNT:\n",
    "            src_pts = np.float32([ kp_model[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "            dst_pts = np.float32([ kp_scene[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "            M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC,5.0)\n",
    "            h,w = model_gray.shape\n",
    "            pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "            \n",
    "            dst = cv.perspectiveTransform(pts,M)\n",
    "\n",
    "            # Compute height as the average of two height measurements\n",
    "            height = int(((dst[1][0][1] - dst[0][0][1]) + (dst[2][0][1] - dst[3][0][1])) / 2)\n",
    "\n",
    "            # Compute width as the average of two width measurements\n",
    "            width = int(((dst[3][0][0] - dst[0][0][0]) + (dst[2][0][0] - dst[1][0][0])) / 2)\n",
    "\n",
    "            # Compute center X coordinate\n",
    "            x = int(((dst[3][0][0] - (dst[3][0][0] - dst[0][0][0]) / 2) +\n",
    "                     (dst[2][0][0] - (dst[2][0][0] - dst[1][0][0]) / 2)) / 2)\n",
    "\n",
    "            # Compute center Y coordinate\n",
    "            y = int(((dst[1][0][1] - (dst[1][0][1] - dst[0][0][1]) / 2) +\n",
    "                     (dst[2][0][1] - (dst[2][0][1] - dst[3][0][1]) / 2)) / 2)\n",
    "\n",
    "            # Create product object and append to result list\n",
    "            results.append(Product((x, y), width, height, dst))\n",
    "        else:\n",
    "            results.append(None)\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "411c8e89ea91e728",
   "metadata": {},
   "source": [
    "### Scene e1"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba2f509f",
   "metadata": {},
   "source": [
    "#define models to be searched in image\n",
    "scene = \"e1.png\"\n",
    "models = [0, 1, 11, 19, 24, 26, 25]\n",
    "\n",
    "result = find_image_A(scene_path=get_path_scene(scene),\n",
    "                    model_paths=[get_path_model(a) for a in models])\n",
    "\n",
    "product_recognition(scene, models, [result])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f29bae99f3f630d2",
   "metadata": {},
   "source": [
    "### Scene e2"
   ]
  },
  {
   "cell_type": "code",
   "id": "5876a54c",
   "metadata": {},
   "source": [
    "#define models to be searched in image\n",
    "scene = \"e2.png\"\n",
    "models = [0, 1, 11, 19, 24, 26, 25]\n",
    "\n",
    "result = find_image_A(scene_path=get_path_scene(scene),\n",
    "                    model_paths=[get_path_model(a) for a in models])\n",
    "\n",
    "product_recognition(scene, models, [result])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e98e876020eb1664",
   "metadata": {},
   "source": [
    "### Scene e3"
   ]
  },
  {
   "cell_type": "code",
   "id": "93a160a2bb2bc6c5",
   "metadata": {},
   "source": [
    "scene = \"e3.png\"\n",
    "models = [0, 1, 11, 19, 24, 26, 25]\n",
    "\n",
    "result = find_image_A(scene_path=get_path_scene(scene),\n",
    "                    model_paths=[get_path_model(a) for a in models])\n",
    "\n",
    "product_recognition(scene, models, [result])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "63826b1b9f188d99",
   "metadata": {},
   "source": [
    "### Scene e4"
   ]
  },
  {
   "cell_type": "code",
   "id": "445b10b030b5132f",
   "metadata": {},
   "source": [
    "scene = \"e4.png\"\n",
    "models = [0, 1, 11, 19, 24, 26, 25]\n",
    "\n",
    "result = find_image_A(scene_path=get_path_scene(scene),\n",
    "                    model_paths=[get_path_model(a) for a in models])\n",
    "\n",
    "product_recognition(scene, models, [result])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6a04a132175b84a4",
   "metadata": {},
   "source": [
    "### Scene e5"
   ]
  },
  {
   "cell_type": "code",
   "id": "c85261189a094edb",
   "metadata": {},
   "source": [
    "scene = \"e5.png\"\n",
    "models = [0, 1, 11, 19, 24, 26, 25]\n",
    "\n",
    "result = find_image_A(scene_path=get_path_scene(scene),\n",
    "                    model_paths=[get_path_model(a) for a in models])\n",
    "\n",
    "product_recognition(scene, models, [result])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step B: step by step\n",
    "\n",
    "In the following cells, we are going to explain our code step by step on an example before defining and executing it all in one function.\n",
    "\n",
    "The next cells defines our scene and model, as well as some parameters that are going to be important later."
   ],
   "id": "99108a82f7e7b504"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#set parameters for demonstration\n",
    "scene = \"m1.png\"\n",
    "scene_path=get_path_scene(scene)\n",
    "models = [0, 1, 11, 19, 24, 25, 26]\n",
    "\n",
    "scene_gray = cv.imread(scene_path,cv.IMREAD_GRAYSCALE)\n",
    "scene_color = cv.imread(scene_path) # for illustrative purposes\n",
    "model_gray = cv.imread(get_path_model(24), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "product_result = []"
   ],
   "id": "ed3fa727474c0b58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Detection of keypoints and matching is the same process as in step A. (Improve???)",
   "id": "313835e6d4de3528"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MIN_MATCH_PERCENTAGE = 13\n",
    "MIN_MATCH_COUNT = 300\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp_model, des1 = sift.detectAndCompute(model_gray, None)\n",
    "kp_scene, des2 = sift.detectAndCompute(scene_gray, None)\n",
    "\n",
    "MIN_MATCH_PERCENTAGE_THRESHOLD = int(len(kp_model) * MIN_MATCH_PERCENTAGE / 100)\n",
    "\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "flann = cv.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# store all the good matches as per Lowe's ratio test.\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "src_kp = [kp_model[m.queryIdx] for m in good]\n",
    "dst_kp = [kp_scene[m.trainIdx] for m in good]"
   ],
   "id": "c1570bda1dbe887a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this subsection, we are going to make use of the Generalized Hough Transformation in combination with Local Invariant Features, which is also called _Star Model_. Instead of an R-table representing the edges of a shape, the GHT + SIFT uses not only the matched features, but also their position relative to a reference point (in this case: the object's barycenter). Those vectors describing the location relative the the barycenter are called _joining vectors_ and should technically all point to the same position (after a transformation is applied). Features which's joining vectors deviate a lot from the most the most common area might be missmatched and can therefore be discarded which improves the model detection. This _area_ is determent by a grid-based voting mechanism.\n",
    "\n",
    "The barycenter P<sub>C</sub> is computed by taking the mean of all found features in the model:\n",
    "\n",
    "P<sub>C</sub> = $\\frac{1}{N}$ $\\sum_{i=1}^N P_i$ , P being the position of the _i_ th feature.\n",
    "\n",
    "The following code computes P<sub>C</sub> and stores the joining vectors in an array:"
   ],
   "id": "d900e51699cad7bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if len(good) >= MIN_MATCH_COUNT or len(good) >= MIN_MATCH_PERCENTAGE_THRESHOLD:\n",
    "    # Compute the barycenter\n",
    "    points = np.array([kp.pt for kp in src_kp], dtype=np.float32)\n",
    "    barycenter = points.mean(axis=0).astype(int)\n",
    "\n",
    "    # compute the joining vectors\n",
    "    points = np.array([kp.pt for kp in src_kp], dtype=np.float32).astype(int)\n",
    "    joining_vectors = barycenter - points"
   ],
   "id": "60e98ac63e4ee275",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this part of the code, for each feature found in the scene the barycenter of the object in the scene is predicted based on the model's joining vector. Because the object in the scene might be scaled and rotated we need to transform the joining vector $V_i$ based on the difference in scale $\\Delta S_j$ and rotation $\\Delta \\varphi_j$ of the feature in the scene. The following formula describes the calculation of the (presumed) barycenter:\n",
    "\n",
    "$\\tilde{\\mathbf{P}}_{C_j} = \\tilde{\\mathbf{P}}_j + \\Delta S_j \\cdot R(\\Delta \\varphi_j) \\mathbf{V}_i$"
   ],
   "id": "733d0e3d9ae94c03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if len(good)>=MIN_MATCH_COUNT or len(good)>=MIN_MATCH_PERCENTAGE_THRESHOLD:\n",
    "\n",
    "    barycenter_votes = []\n",
    "\n",
    "    for s, d, jv in zip(src_kp, dst_kp, joining_vectors):\n",
    "\n",
    "        jv_x, jv_y = jv\n",
    "\n",
    "        pt_x, pt_y = d.pt\n",
    "        delta_s = d.size/s.size\n",
    "\n",
    "        delta_phi = d.angle - s.angle\n",
    "\n",
    "        angle_radians = math.radians(delta_phi)\n",
    "        jv_x_rot = jv_x * math.cos(angle_radians) - jv_y * math.sin(angle_radians)\n",
    "        jv_y_rot = jv_x * math.sin(angle_radians) + jv_y * math.cos(angle_radians)\n",
    "\n",
    "        bary_predict = [int(pt_x + delta_s * jv_x_rot), int(pt_y + delta_s * jv_y_rot)]\n",
    "        barycenter_votes.append(bary_predict)"
   ],
   "id": "623f4a658ee1b965",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This cell implements the voting. The voting assumes that correctly matched features describe the barycenter at the same position. A 25x25px grid is laid over the image, seperating it in bins. For a feature to be considered its joining vector need to point to a bin with at least 25 votes. This number has proofed well in our testing. The figure shows all matches and where their joining vectors are pointing at. Also, for each winning bin an average of all predicted barycenters is saved in an array called _barygrid_means_ which will be relevant later.",
   "id": "93fb5be3259f8ae3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "GRID_RESOLUTION=25\n",
    "MIN_VOTES=25\n",
    "\n",
    "\n",
    "x_cell = int(scene_gray.shape[1]/GRID_RESOLUTION)\n",
    "y_cell = x_cell\n",
    "\n",
    "y_cells_n = int(scene_gray.shape[1]//y_cell + 1)\n",
    "\n",
    "# Initialize a 2D array where each element is an empty list\n",
    "grid = [[[] for _ in range(GRID_RESOLUTION)] for _ in range(y_cells_n)]\n",
    "\n",
    "barygrid = [[[] for _ in range(GRID_RESOLUTION)] for _ in range(y_cells_n)]\n",
    "\n",
    "# assign features matches and barycenters to grids\n",
    "for i in range(0,len(barycenter_votes)):\n",
    "    bary = barycenter_votes[i]\n",
    "    # Check if predict Bary center is in Picture\n",
    "    if 0 < bary[0] < scene_gray.shape[1] and 0 < bary[1] < scene_gray.shape[0]:\n",
    "        x_idx = int(bary[0]/x_cell)\n",
    "        y_idx = int(bary[1]/y_cell)\n",
    "        grid[x_idx][y_idx].append((src_kp[i], dst_kp[i]))\n",
    "        barygrid[x_idx][y_idx].append(bary)\n",
    "\n",
    "draw_barycenter_predictions(scene_color.copy(), x_cell, y_cell, barycenter_votes, dst_kp)\n",
    "\n",
    "# select vote winners in grid\n",
    "barygrid_means = []\n",
    "winners = []\n",
    "for i in range(0,len(grid)):\n",
    "    for j in range(0,len(grid[i])):\n",
    "        if len(grid[i][j])>MIN_VOTES:\n",
    "            winners.append(grid[i][j])\n",
    "            barygrid_means.append(np.mean(barygrid[i][j], axis=0))"
   ],
   "id": "6008af3e8b056d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The upper part of the next cell makes use of __barygrid_means__, which contains the mean value of bins who won in the voting, by checking if it lies in an area which must be ignored. Ignored areas are those where an object of the same type has already been found. This is necessary, since a dense cloud of predicted barycenters could be cut in half during the voting depending on the placment of the grid. To prevent looking for the same object twice or thrice, as soon as on object was recognized, we prevent the code from looking for in again in the same area. Looking for the same instance in other places of the scene is still possible.",
   "id": "b6bd806d167dae57"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The lower part uses our filterd matched feature to calculate the the perspective transformation of the model, its homography, by using __cv.findHomography()__. We then use __cv.perspectiveTransform()__ to lay its corners on the scene to visualize it. Since the task suggests a need for the shapes to be rectangular, we adjust them for the wanted shape and add them to the list of results. #TODO hier vllt noch mehr dazu?",
   "id": "7e0fd1090e60db2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "counter = 0\n",
    "\n",
    "ignored_area = []\n",
    "for winner, barygrid_mean in zip(winners, barygrid_means):\n",
    "\n",
    "\n",
    "    # check if Winner is in area with already found winner\n",
    "    winner_found = False\n",
    "    for rec in ignored_area:\n",
    "        if is_point_in_rectangle(barygrid_mean, rec):\n",
    "            winner_found = True\n",
    "            break\n",
    "\n",
    "    if winner_found:\n",
    "        continue\n",
    "\n",
    "    print(\"here\")\n",
    "    #USE THIS TO DRAW THE RESULTS FROM POINTS\n",
    "    src_pts = np.float32([ kp[0].pt for kp in winner ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp[1].pt for kp in winner ]).reshape(-1,1,2)\n",
    "\n",
    "    #draw the final matched features to be considered to calculate the homography with\n",
    "    tmp = scene_color.copy()\n",
    "    for coord in dst_pts:\n",
    "        cv.circle(tmp, np.array([o for o in coord[0]], dtype=np.int64), 2, (0,255,0), 5)\n",
    "\n",
    "    plt.imshow(cv.cvtColor(tmp, cv.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Final matched features to be considered for instance {counter}\")\n",
    "    plt.show()\n",
    "\n",
    "    counter+=1\n",
    "\n",
    "    M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC,5.0)\n",
    "    h,w = model_gray.shape\n",
    "    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "\n",
    "    dst = cv.perspectiveTransform(pts,M)\n",
    "\n",
    "    # Compute height as the average of two height measurements\n",
    "    height = int(((dst[1][0][1] - dst[0][0][1]) + (dst[2][0][1] - dst[3][0][1])) / 2)\n",
    "\n",
    "    # Compute width as the average of two width measurements\n",
    "    width = int(((dst[3][0][0] - dst[0][0][0]) + (dst[2][0][0] - dst[1][0][0])) / 2)\n",
    "\n",
    "    # Compute center X coordinate\n",
    "    x = int(((dst[3][0][0] - (dst[3][0][0] - dst[0][0][0]) / 2) +\n",
    "                (dst[2][0][0] - (dst[2][0][0] - dst[1][0][0]) / 2)) / 2)\n",
    "\n",
    "    # Compute center Y coordinate\n",
    "    y = int(((dst[1][0][1] - (dst[1][0][1] - dst[0][0][1]) / 2) +\n",
    "                (dst[2][0][1] - (dst[2][0][1] - dst[3][0][1]) / 2)) / 2)\n",
    "\n",
    "    # Create product object and append to result list\n",
    "    product_result.append(Product((x, y), width, height, dst))\n",
    "\n",
    "    #add found instance to ignored areas\n",
    "    ignored_area.append([o[0] for o in np.int32(dst)])\n"
   ],
   "id": "d74617c768409b16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models = [24]\n",
    "product_recognition(scene, models, [product_result])"
   ],
   "id": "7af764450679ccf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step B: As Function\n",
    "\n",
    "The following cell takes all of our previoursly discussed code snippets and merges them into one functio taking into account to detecting multiple models."
   ],
   "id": "c14b00c8c31b5136"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_image(scene_path, model_paths, MIN_MATCH_PERCENTAGE = 13, MIN_MATCH_COUNT=300, GRID_RESOLUTION=25, MIN_VOTES=25):\n",
    "    scene_gray = cv.imread(scene_path,cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    #create array with all models\n",
    "    models = []\n",
    "    for model_gray_path in model_paths:\n",
    "\n",
    "        name = os.path.split(model_gray_path)[-1]\n",
    "        models.append({\n",
    "            \"name\": name,\n",
    "            \"img\": cv.imread(model_gray_path, cv.IMREAD_GRAYSCALE)\n",
    "            })\n",
    "\n",
    "    for model in models:\n",
    "        product_result = []\n",
    "\n",
    "        # Apply model preprocessing\n",
    "        model_gray = model[\"img\"]\n",
    "\n",
    "        # Initiate SIFT detector\n",
    "        sift = cv.SIFT_create()\n",
    "\n",
    "        # find the keypoints and descriptors with SIFT\n",
    "        kp_model, des1 = sift.detectAndCompute(model_gray,None)\n",
    "        kp_scene, des2 = sift.detectAndCompute(scene_gray,None)\n",
    "\n",
    "        MIN_MATCH_PERCENTAGE_THRESHOLD = int(len(kp_model) * MIN_MATCH_PERCENTAGE/100)\n",
    "\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks = 50)\n",
    "        flann = cv.FlannBasedMatcher(index_params, search_params)\n",
    "        matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "        # store all the good matches as per Lowe's ratio test.\n",
    "        good = []\n",
    "        for m,n in matches:\n",
    "            if m.distance < 0.7 * n.distance:\n",
    "                good.append(m)\n",
    "\n",
    "        src_kp = [kp_model[m.queryIdx] for m in good]\n",
    "        dst_kp = [kp_scene[m.trainIdx] for m in good]\n",
    "\n",
    "        if len(good)>=MIN_MATCH_COUNT or len(good)>=MIN_MATCH_PERCENTAGE_THRESHOLD:\n",
    "            #calculate barycenter of model\n",
    "            x = 0\n",
    "            y = 0\n",
    "            for kp in src_kp:\n",
    "                x_kp, y_kp = kp.pt\n",
    "                x += x_kp\n",
    "                y += y_kp\n",
    "            barycenter = np.array([int(x/len(src_kp)), int(y/len(src_kp))])\n",
    "\n",
    "            # calculate joining vectors of model\n",
    "            joining_vectors = []\n",
    "            for kp in src_kp:\n",
    "                x, y = kp.pt\n",
    "                v = barycenter - [int(x), int(y)]\n",
    "                joining_vectors.append(v)\n",
    "\n",
    "            barycenter_votes = []\n",
    "\n",
    "            for s, d, jv in zip(src_kp, dst_kp, joining_vectors):\n",
    "\n",
    "                if jv is None:\n",
    "                    raise Exception(\"there must be the same point in\")\n",
    "\n",
    "                jv_x, jv_y = jv\n",
    "\n",
    "                pt_x, pt_y = d.pt\n",
    "                delta_s = d.size/s.size\n",
    "\n",
    "                delta_phi = d.angle - s.angle\n",
    "\n",
    "                angle_radians = math.radians(delta_phi)\n",
    "                jv_x_rot = jv_x * math.cos(angle_radians) - jv_y * math.sin(angle_radians)\n",
    "                jv_y_rot = jv_x * math.sin(angle_radians) + jv_y * math.cos(angle_radians)\n",
    "\n",
    "                bary_predict = [int(pt_x + delta_s * jv_x_rot), int(pt_y + delta_s * jv_y_rot)]\n",
    "                barycenter_votes.append(bary_predict)\n",
    "\n",
    "            x_cell = int(scene_gray.shape[1]/GRID_RESOLUTION)\n",
    "            y_cell = x_cell\n",
    "\n",
    "            y_cells_n = int(scene_gray.shape[1]//y_cell + 1)\n",
    "\n",
    "            # Initialize a 2D array where each element is an empty list\n",
    "            grid = [[[] for _ in range(GRID_RESOLUTION)] for _ in range(y_cells_n)]\n",
    "\n",
    "            barygrid = [[[] for _ in range(GRID_RESOLUTION)] for _ in range(y_cells_n)]\n",
    "\n",
    "            # assign features matches and barycenters to grids\n",
    "            for i in range(0,len(barycenter_votes)):\n",
    "                bary = barycenter_votes[i]\n",
    "                # Check if predict Bary center is in Picture\n",
    "                if 0 < bary[0] < scene_gray.shape[1] and 0 < bary[1] < scene_gray.shape[0]:\n",
    "                    x_idx = int(bary[0]/x_cell)\n",
    "                    y_idx = int(bary[1]/y_cell)\n",
    "                    grid[x_idx][y_idx].append((src_kp[i], dst_kp[i]))\n",
    "                    barygrid[x_idx][y_idx].append(bary)\n",
    "            # select vote winners in grid\n",
    "            barygrid_means = []\n",
    "            winners = []\n",
    "\n",
    "            for i in range(0,len(grid)):\n",
    "                for j in range(0,len(grid[i])):\n",
    "                    if len(grid[i][j])>MIN_VOTES:\n",
    "                        winners.append(grid[i][j])\n",
    "                        barygrid_means.append(np.mean(barygrid[i][j], axis=0))\n",
    "\n",
    "            ignored_area = []\n",
    "            for winner, barygrid_mean in zip(winners, barygrid_means):\n",
    "\n",
    "                # check if Winner is in area with already found winner\n",
    "                winner_found = False\n",
    "                for rec in ignored_area:\n",
    "                    if is_point_in_rectangle(barygrid_mean, rec):\n",
    "                        winner_found = True\n",
    "                        break\n",
    "\n",
    "                if winner_found:\n",
    "                    continue\n",
    "                #USE THIS TO DRAW THE RESULTS FROM POINTS\n",
    "                src_pts = np.float32([ kp[0].pt for kp in winner ]).reshape(-1,1,2)\n",
    "                dst_pts = np.float32([ kp[1].pt for kp in winner ]).reshape(-1,1,2)\n",
    "\n",
    "                M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC,5.0)\n",
    "                h,w = model_gray.shape\n",
    "                pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "\n",
    "                dst = cv.perspectiveTransform(pts,M)\n",
    "\n",
    "                # Compute height as the average of two height measurements\n",
    "                height = int(((dst[1][0][1] - dst[0][0][1]) + (dst[2][0][1] - dst[3][0][1])) / 2)\n",
    "\n",
    "                # Compute width as the average of two width measurements\n",
    "                width = int(((dst[3][0][0] - dst[0][0][0]) + (dst[2][0][0] - dst[1][0][0])) / 2)\n",
    "\n",
    "                # Compute center X coordinate\n",
    "                x = int(((dst[3][0][0] - (dst[3][0][0] - dst[0][0][0]) / 2) +\n",
    "                         (dst[2][0][0] - (dst[2][0][0] - dst[1][0][0]) / 2)) / 2)\n",
    "\n",
    "                # Compute center Y coordinate\n",
    "                y = int(((dst[1][0][1] - (dst[1][0][1] - dst[0][0][1]) / 2) +\n",
    "                         (dst[2][0][1] - (dst[2][0][1] - dst[3][0][1]) / 2)) / 2)\n",
    "\n",
    "                # Create product object and append to result list\n",
    "                product_result.append(Product((x, y), width, height, dst))\n",
    "\n",
    "                #add found instance to ignored areas\n",
    "                ignored_area.append([o[0] for o in np.int32(dst)])\n",
    "        result.append(product_result)\n",
    "    return result"
   ],
   "id": "68504229c6dc1f15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#define models to be searched in image\n",
    "scene = \"m1.png\"\n",
    "models = [0, 1, 11, 19, 24, 25, 26]\n",
    "\n",
    "result = find_image(scene_path=get_path_scene(scene),\n",
    "                    model_paths=[get_path_model(a) for a in models])\n",
    "\n",
    "product_recognition(scene,result)"
   ],
   "id": "8ce32efe32a43a5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#define models to be searched in image\n",
    "scene = \"m2.png\"\n",
    "models = [0, 1, 11, 19, 24, 25, 26]\n",
    "\n",
    "result = find_image(scene_path=get_path_scene(scene),\n",
    "                    model_paths=[get_path_model(a) for a in models])\n",
    "\n",
    "product_recognition(scene,result)"
   ],
   "id": "5cfc2854849a6611"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#define models to be searched in image\n",
    "scene = \"m3.png\"\n",
    "models = [0, 1, 11, 19, 24, 25, 26]\n",
    "\n",
    "result = find_image(scene_path=get_path_scene(scene),\n",
    "                    model_paths=[get_path_model(a) for a in models])\n",
    "\n",
    "product_recognition(scene,result)"
   ],
   "id": "a55dbf132ac07cc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#define models to be searched in image\n",
    "scene = \"m4.png\"\n",
    "models = [0, 1, 11, 19, 24, 25, 26]\n",
    "\n",
    "result = find_image(scene_path=get_path_scene(scene),\n",
    "                    model_paths=[get_path_model(a) for a in models])\n",
    "\n",
    "product_recognition(scene,result)"
   ],
   "id": "71eb2ae8ae38c913"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#define models to be searched in image\n",
    "scene = \"m5.png\"\n",
    "models = [0, 1, 11, 19, 24, 25, 26]\n",
    "\n",
    "result = find_image(scene_path=get_path_scene(scene),\n",
    "                    model_paths=[get_path_model(a) for a in models])\n",
    "\n",
    "product_recognition(scene,result)"
   ],
   "id": "eaec514031fd4490"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step C\n",
    "\n",
    "Step C runs on the same code as Step B does, but we refiened the parameters even more to work for the more challenging scenes. We successfully recognized some of the products but recognizing all was not possible, partly because the number of features detected in the scenes were very low for each product, because the scene's resolution were relativly bad.\n",
    "Since we could not find a dataset containing cereal boxes, machine learning / deep learning approaches were also not possible."
   ],
   "id": "9487abfad9b0846"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b5e4d9b72ae96914"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
